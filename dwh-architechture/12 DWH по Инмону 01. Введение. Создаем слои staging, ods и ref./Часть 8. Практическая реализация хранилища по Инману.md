## **📌 8.1 Что такое ETL?**

**ETL (Extract, Transform, Load)** – процесс **извлечения данных из источников, их обработки и загрузки в DWH**.

### **🔹 Основные этапы ETL**

1️⃣ **Extract (Извлечение данных)**

- Получение данных из **ERP, CRM, API, файлов (CSV, JSON, XML, Parquet)**.
- Работа с **реляционными (PostgreSQL, MySQL, Oracle) и NoSQL (MongoDB, Redis, ClickHouse) базами данных**.

2️⃣ **Transform (Трансформация данных)**

- Очистка, нормализация, удаление дубликатов.
- Обогащение данных, создание суррогатных ключей.
- Агрегация данных, расчеты показателей.

3️⃣ **Load (Загрузка в хранилище)**

- Запись данных в **ODS, Reference, Integration, DDS**.
- Историзация данных (SCD Type 2).

📌 **Как выглядит ETL-процесс?**

```
[ Источники данных (ERP, CRM, API) ]
        ↓ (Extract)
[ Staging Layer (сырые данные) ]
        ↓ (Transform)
[ ODS Layer (операционные данные) ]
        ↓ (Transform)
[ Reference Layer (нормализация ключей) ]
        ↓ (Transform)
[ DDS Layer (исторические данные) ]
        ↓ (Load)
[ Data Marts (отчеты для BI) ]
```

✅ **ETL-пайплайн проходит через все слои хранилища.**

---

## **📌 8.2 Инструменты для ETL**

Рассмотрим основные **инструменты для автоматизации ETL-процессов**.

|**Инструмент**|**Описание**|
|---|---|
|**Apache Airflow**|Управление ETL-пайплайнами (Python)|
|**Apache Nifi**|Визуальный ETL-конвейер|
|**Talend**|Графический ETL-инструмент|
|**DBT (Data Build Tool)**|SQL-платформа для обработки данных|
|**Python (pandas, psycopg2, SQLAlchemy)**|Программирование ETL-процессов|
|**SQL (PL/pgSQL, Stored Procedures)**|Встроенные SQL-скрипты в DWH|

✅ **Лучший выбор для нашего примера:**

- **PostgreSQL (хранимые процедуры) → Python (ETL-скрипты) → Apache Airflow (автоматизация).**

---

## **📌 8.3 Разработка ETL для Staging Layer (сырые данные)**

📌 **Задача:** Загружать сырые данные из источников в **Staging Layer**.

📊 **Пример ETL в SQL (PostgreSQL)**

```sql
CREATE TABLE staging.sales_raw (
    id SERIAL PRIMARY KEY,
    source_system VARCHAR(50), -- CRM, ERP, API
    data JSONB, -- Исходные данные в JSON
    load_timestamp TIMESTAMP DEFAULT now()
);
```

✅ **Сырые данные хранятся в формате JSON, чтобы сохранять структуру источников.**

📌 **Python-скрипт для загрузки данных из API в Staging Layer**

```python
import requests
import psycopg2
import json

# Подключение к PostgreSQL
conn = psycopg2.connect("dbname=dwh user=etl password=secret")
cur = conn.cursor()

# Запрос к API
response = requests.get("https://api.salesdata.com/orders")
data = response.json()

# Запись данных в Staging
for record in data:
    cur.execute("INSERT INTO staging.sales_raw (source_system, data) VALUES (%s, %s)",
                ('CRM', json.dumps(record)))

conn.commit()
cur.close()
conn.close()
```

✅ **Результат:** Данные из API загружаются в **Staging Layer**.

---

## **📌 8.4 Разработка ETL для ODS Layer (операционные данные)**

📌 **Задача:**

- Очистить сырые данные.
- Развернуть JSON в таблицы.
- Удалить дубликаты.

📊 **Пример SQL-трансформации данных из Staging в ODS**

```sql
CREATE TABLE ods.sales (
    sale_id INT PRIMARY KEY,
    customer_id INT,
    product_id INT,
    amount DECIMAL(10,2),
    sale_date TIMESTAMP
);

INSERT INTO ods.sales (sale_id, customer_id, product_id, amount, sale_date)
SELECT 
    (data->>'id')::INT AS sale_id,
    (data->>'customer_id')::INT AS customer_id,
    (data->>'product_id')::INT AS product_id,
    (data->>'amount')::DECIMAL(10,2) AS amount,
    (data->>'sale_date')::TIMESTAMP AS sale_date
FROM staging.sales_raw;
```

✅ **Результат:** **ODS содержит "чистые" данные в структурированном виде.**

---

## **📌 8.5 Разработка ETL для Reference Layer (нормализация идентификаторов)**

📌 **Задача:**

- Создать **единые ID клиентов и продуктов**.
- Исключить дубли из разных систем.

📊 **Пример SQL для Reference Layer**

```sql
CREATE TABLE ref.customers (
    customer_sk SERIAL PRIMARY KEY,
    source_id INT UNIQUE,
    full_name VARCHAR(255)
);

INSERT INTO ref.customers (source_id, full_name)
SELECT DISTINCT customer_id, CONCAT(first_name, ' ', last_name)
FROM ods.sales;
```

✅ **Результат:** У каждого клиента теперь есть **глобальный идентификатор (customer_sk)**.

---

## **📌 8.6 Разработка ETL для DDS Layer (исторические данные)**

📌 **Задача:**

- **Добавлять новые версии данных**, если значения изменились.
- **Использовать метод SCD Type 2** для хранения истории.

📊 **Пример SQL для хранения истории продаж**

```sql
CREATE TABLE dds.sales_history (
    id SERIAL PRIMARY KEY,
    customer_sk INT,
    product_id INT,
    amount DECIMAL(10,2),
    status VARCHAR(50),
    valid_from TIMESTAMP,
    valid_to TIMESTAMP NULL DEFAULT NULL
);
```

📌 **Хранимая процедура для добавления истории (SCD Type 2)**

```sql
CREATE OR REPLACE FUNCTION update_sales_history()
RETURNS VOID AS $$
BEGIN
    -- Завершаем старую запись
    UPDATE dds.sales_history
    SET valid_to = now()
    WHERE customer_sk IN (SELECT customer_sk FROM ref.customers)
    AND valid_to IS NULL;

    -- Вставляем новую версию данных
    INSERT INTO dds.sales_history (customer_sk, product_id, amount, status, valid_from)
    SELECT 
        customer_sk, product_id, amount, status, now()
    FROM ods.sales;
END $$ LANGUAGE plpgsql;
```

✅ **Результат:** **Все изменения данных сохраняются, можно отслеживать историю.**

---

## **📌 8.7 Автоматизация ETL через Apache Airflow**

📌 **Задача:**

- Автоматизировать выполнение всех ETL-скриптов по расписанию.
- Обеспечить надежность ETL-пайплайна.

📊 **Пример DAG в Apache Airflow**

```python
from airflow import DAG
from airflow.operators.bash_operator import BashOperator
from datetime import datetime

default_args = {
    'owner': 'etl_team',
    'start_date': datetime(2024, 1, 1),
    'retries': 1
}

dag = DAG(
    'dwh_etl_pipeline',
    default_args=default_args,
    schedule_interval='@daily'
)

extract_data = BashOperator(
    task_id='extract_data',
    bash_command='python extract.py',
    dag=dag
)

transform_load = BashOperator(
    task_id='transform_load',
    bash_command='psql -f transform_load.sql',
    dag=dag
)

extract_data >> transform_load
```

✅ **Результат:** **ETL автоматически выполняется каждый день.**

---

## **📌 Итоги**

🎯 **Теперь у нас есть полный ETL для хранилища по Инману:**  
✅ **Staging Layer** – сырые данные.  
✅ **ODS Layer** – операционные данные.  
✅ **Reference Layer** – нормализация.  
✅ **DDS Layer** – исторические данные.  
✅ **Автоматизация через Apache Airflow.**

🔜 **Следующий шаг** → **Создание Data Marts и BI-аналитики (Power BI, Tableau, Grafana).** 🚀