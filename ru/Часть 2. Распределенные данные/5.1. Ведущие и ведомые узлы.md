Узлы, хранящие копии базы данных, называются репликами. Основной задачей является обеспечение согласованности данных во всех репликах, что достигается с помощью **репликации с ведущим узлом** (leader-based replication).

В этой модели одна реплика становится ведущей (leader), принимая запросы на запись от клиентов и записывая данные в своё хранилище. Затем ведущий отправляет журнал изменений (replication log) всем ведомым узлам (followers), которые обновляют свои копии базы данных.

![[22. Replication with leading node.png]]

Чтение данных возможно как с ведущего узла, так и с ведомых, но записи разрешены только на ведущем. Этот метод поддерживается многими реляционными и нереляционными базами данных, такими как PostgreSQL, MySQL и MongoDB.

### Синхронная и асинхронная репликация

Важным аспектом работы реплицируемых систем является выбор между синхронной и асинхронной репликацией. При синхронной репликации ведущий узел дожидается подтверждения от ведомого узла перед тем, как сообщить клиенту об успешном обновлении данных. Это обеспечивает актуальность данных, но может замедлить систему, если ведомый узел не отвечает. Асинхронная репликация, наоборот, позволяет ведущему узлу отправлять изменения ведомым без ожидания подтверждения, что увеличивает скорость операций, но может привести к потере данных при сбое ведущего узла.

![[23. Async and sync replication.png]]

На практике часто используется полусинхронная конфигурация, где один из ведомых узлов синхронный, а остальные асинхронные. Это позволяет гарантировать наличие актуальных данных на хотя бы двух узлах. Полностью асинхронные системы удобны при наличии множества ведомых узлов, но могут привести к потере данных в случае сбоя ведущего узла.

Исследователи продолжают изучать методы, которые обеспечивают сохранность данных и хорошую производительность, такие как цепная репликация. В данной главе акцент сделан на более простых и широко используемых формах репликации в базах данных.

### Создание новых ведомых узлов

При создании новых ведомых узлов для увеличения числа реплик или замены сбойных важно обеспечить согласованность данных. Простое копирование данных не подходит, поскольку база постоянно обновляется. Вместо этого процесс включает следующие шаги:

1. Создание согласованного снимка состояния базы данных ведущего узла без блокировки системы, используя встроенные функции или сторонние утилиты (например, innobackupex для MySQL).
2. Копирование этого снимка на новый ведомый узел.
3. Подключение ведомого узла к ведущему и запрос изменений, произошедших с момента создания снимка, используя позицию в журнале репликации (например, регистрационный номер транзакции в PostgreSQL).

Этот процесс может варьироваться в зависимости от системы: некоторые базы данных предлагают автоматизированные решения, в то время как другие требуют ручных действий от администратора.
### Перебои в обслуживании узлов

Для обеспечения высокой доступности при репликации с одним ведущим узлом важно минимизировать влияние отказов или технического обслуживания узлов. Система должна продолжать функционировать, позволяя при этом перезагрузку отдельных узлов без простоя.

### Отказ ведомого узла: наверстывающее восстановление

Каждый ведомый узел хранит журнал изменений, полученных от ведущего узла. При сбое или перезагрузке ведомый узел может восстановить работу, обращаясь к своему журналу. Он подключается к ведущему и запрашивает все изменения, произошедшие за время его недоступности, что позволяет ему синхронизироваться и продолжать обработку данных.

### Отказ ведущего узла: восстановление после отказа

Восстановление после отказа (failover) — это процесс, который осуществляется при сбое ведущего узла в распределенной системе. Он включает в себя несколько этапов:

1. **Установление отказа**: Система определяет, что ведущий узел недоступен, обычно через механизм превышения времени ожидания. Если узел не отвечает в течение установленного времени (например, 30 секунд), он считается неработоспособным.
    
2. **Выбор нового ведущего узла**: Это может быть сделано через процесс «выборов», где новый ведущий выбирается среди оставшихся реплик, либо с помощью заранее назначенного узла-контроллера. Оптимальным кандидатом считается реплика с самыми свежими данными, чтобы минимизировать потери.
    
3. **Настройка системы**: Клиенты должны перенастроиться на новый ведущий узел. Если старый узел возвращается в сеть, он должен осознать, что его статус изменился на ведомый.
    

Процесс восстановления связан с множеством проблем. Например, при асинхронной репликации новый ведущий может не получить все записи от старого, что приводит к потенциальным конфликтам. В одном случае на GitHub ведомый узел с устаревшими данными был повышен до ведущего, что вызвало несогласованность данных между MySQL и Redis.

Также существует риск «разделения вычислительных мощностей», когда два узла считают себя ведущими. Это может привести к потерям данных без эффективного механизма разрешения конфликтов. Необходимо тщательно проектировать систему, чтобы избежать таких ситуаций.

Определение времени ожидания для объявления ведущего узла недоступным также критично: слишком долгое ожидание увеличивает время восстановления, в то время как слишком короткое может вызвать ложные срабатывания. Поэтому некоторые администраторы предпочитают ручное восстановление, несмотря на возможности автоматизации. Эти вопросы являются фундаментальными проблемами распределенных систем и будут рассмотрены подробнее в следующих главах.