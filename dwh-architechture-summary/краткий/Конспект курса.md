# Конспект курса "Архитектура хранилищ данных"

Современные организации сталкиваются с необходимостью объединения данных из множества разрозненных источников для построения аналитических отчетов. Традиционные операционные базы данных не справляются с аналитическими запросами, которые создают значительную нагрузку и замедляют работу систем. Решение этой проблемы — создание отдельного хранилища данных (Data Warehouse, DWH), которое объединяет информацию из разных источников, разгружает операционные базы и позволяет быстро строить отчеты.

## Основы архитектуры хранилищ данных

Хранилище данных не просто копирует информацию из исходных баз, а структурирует её по уровням. В классической архитектуре выделяют три основных слоя: Staging (промежуточный слой для сырых данных), Core (основное хранилище с обработанными данными) и Data Marts (витрины данных для конкретных отчетов). Staging Layer принимает данные в том виде, в котором они есть в источнике, без преобразований. Core Layer обеспечивает чистоту, целостность и нормализацию данных, приводя их к единому формату. Marts Layer содержит готовые таблицы для аналитики и отчетности с агрегированными данными.

Создание хранилища начинается с настройки Staging Layer. Создается отдельная база данных для DWH и схема `staging` внутри неё. Для подключения к базе данных источника используется PostgreSQL FDW (Foreign Data Wrapper) — современный способ подключения к удаленным серверам. Настраивается серверное соединение и маппинг пользователей. Важно заменить специфические типы данных источника (например, ENUM, пользовательские типы) на стандартные типы (VARCHAR) в хранилище, так как эти типы могут быть неподдерживаемыми.

Для загрузки данных в Staging можно использовать два подхода: TRUNCATE + INSERT (полная перезагрузка, простая в реализации) и DELETE + INSERT (инкрементальная загрузка, требует механизма определения изменений). На начальном этапе используется полная перезагрузка. Создаются процедуры для загрузки данных из источника в Staging Layer. Для автоматизации создается общая процедура, которая запускает все загрузки, и настраивается автоматическое выполнение через `pg_cron` (например, каждый день в 2:00 ночи).

Ключевое правило работы со слоями: Staging = копия источника (никаких преобразований), Core = обработанные данные (после валидации и нормализации). В Staging-слое данные записываются без изменений, как есть в источнике. В основном хранилище данные обрабатываются, очищаются и нормализуются.

## Нормализация и структурирование данных

После создания Staging Layer данные переносятся в Core Layer — основной слой хранилища с нормализованными и обработанными данными. Нормализация — это процесс организации данных в базе так, чтобы избежать дублирования информации, обеспечить согласованность данных и повысить гибкость хранения. Избыточность данных возникает, когда одна и та же информация хранится в нескольких местах, что приводит к несогласованности при изменениях.

Первая нормальная форма (1NF) требует, чтобы в одной ячейке было только одно значение. Нарушение 1NF приводит к сложностям при поиске по отдельным значениям, неработоспособности индексов и невозможности создания внешних ключей. Данные должны быть атомарными — разбиты на минимальные логические части.

Вторая нормальная форма (2NF) требует, чтобы таблица была в 1NF, и если в таблице есть составной ключ, то все поля должны зависеть от него целиком, а не от части ключа. Нарушение 2NF приводит к избыточности данных, когда некоторые поля зависят только от части ключа. Решение — выделение отдельных таблиц для зависимых сущностей.

Третья нормальная форма (3NF) требует, чтобы таблица была в 2NF, и каждое поле должно зависеть только от первичного ключа, а не от других полей. Нарушение 3NF приводит к необходимости обновления данных в нескольких местах при изменении зависимых значений. Решение — создание отдельных таблиц для зависимых сущностей с ссылками на них.

В Core Layer выполняется очистка данных: удаление дубликатов, замена NULL значений на значения по умолчанию, нормализация (разделение данных на отдельные таблицы). Создаются ETL-процедуры для переноса данных из Staging в Core. Для оптимизации производительности добавляются индексы на ключевые поля и используется партиционирование таблиц для больших объемов данных. Автоматизируется процесс загрузки через общие процедуры и настройку выполнения по расписанию.

## Многомерное моделирование по Кимбалу

Методология Кимбалла использует многомерное моделирование данных с использованием схемы "звезда" (star schema). В схеме "звезда" факты находятся в центре, окруженные денормализованными измерениями. Альтернативная схема "снежинка" (snowflake schema) предполагает нормализацию измерений и их разбиение на подтаблицы. Для хранилищ данных по Кимбалу предпочтительна схема "звезда" из-за простоты и производительности запросов.

Создается схема `core` в PostgreSQL. Создаются таблицы измерений (dimension tables), например `dim_inventory` для информации о фильмах и дисках, `dim_staff` для информации о сотрудниках и магазинах. Измерения содержат суррогатные ключи (например, `inventory_pk`), идентификаторы из источника (например, `inventory_id`, `film_id`) и атрибуты сущности (название, рейтинг, стоимость и т.д.).

Создаются таблицы фактов (fact tables), например `fact_payment` для оплаты аренды, `fact_rental` для фактов аренды. Факты содержат суррогатные ключи, ссылки на измерения через внешние ключи, даты событий и метрики (суммы, количества), которые можно агрегировать. Формируется структура схемы "звезда" с фактами в центре и измерениями вокруг.

Наполнение Core-слоя происходит через полную перезагрузку данных. Сначала перегружаются данные из источника в стейджинг-слой, затем из стейджинг-слоя в Core-слой. Данные из Staging соединяются с измерениями Core для получения суррогатных ключей, группируются по измерениям и датам, и загружаются в таблицы фактов. Проверяется корректность загрузки через сравнение количества записей.

## Историчность данных и инкрементальная загрузка

По мере развития хранилища возникает необходимость отслеживания изменений данных во времени. Slow Changing Dimension Type 2 (SCD2) — это метод поддержки историчности в измерениях, при котором при изменении атрибута создается новая запись с новым периодом действия, а старая запись помечается как неактивная. Это позволяет отслеживать, как изменялись данные во времени, и строить отчеты на исторических срезах.

В денормализованных измерениях историчность поддерживается через добавление полей `valid_from` и `valid_to` для определения периода действия записи. При изменении атрибута создается новая запись с обновленными данными и новым периодом действия. Старая запись остается в базе для исторического анализа.

Инкрементальная загрузка измерений позволяет загружать только измененные записи вместо полной перезагрузки. Определяются новые записи (отсутствующие в Core), измененные записи (с измененными атрибутами) и удаленные записи (отсутствующие в источнике). Для новых записей создаются новые строки в измерениях. Для измененных записей создаются новые версии с историчностью. Для удаленных записей обновляется статус или период действия.

Инкрементальная загрузка в таблице фактов требует определения новых фактов (новые события) и измененных фактов (исправления в данных). Новые факты добавляются в таблицу. Измененные факты могут обновляться или создаваться как новые версии в зависимости от бизнес-логики. Важно учитывать гранулярность данных и возможность дублирования при инкрементальной загрузке.

## Измерение дат и витрины данных

Создание измерения дат (date dimension) является важной частью хранилища данных. Таблица дат содержит все даты за определенный период (например, несколько лет), с предварительно вычисленными атрибутами: день недели, месяц, квартал, год, праздничные дни, рабочие дни и т.д. Это упрощает анализ данных по временным периодам и повышает производительность запросов.

Витрины данных (Data Marts) — это оптимизированные наборы данных для построения конкретных отчетов. Витрины содержат агрегированные данные, готовые для анализа, и разгружают основное хранилище. Создается схема `mart` для изоляции витрин. Разрабатываются отчеты, например `mart.revenue_report` для выручки по месяцам с расчетом прибыли, `mart.category_sales` для продаж по категориям. Для каждого отчета создаются процедуры загрузки данных из Core Layer с агрегацией.

Автоматизируется обновление витрин через общие процедуры и настройку выполнения по расписанию. Витрины готовы для подключения к BI-системам (Power BI, Tableau, Metabase) для визуализации данных. OLAP-кубы — это специализированные структуры данных, оптимизированные для анализа, содержащие факты и измерения. В современных системах кубы используются реже благодаря улучшенной производительности баз данных, но всё ещё полезны в узкоспециализированных задачах.

## Методология Инмона

Модель Инмона (Inmon Data Warehouse) — альтернативный подход к построению хранилищ данных. В отличие от Кимбалла, строится в третьей нормальной форме (3NF). Основная идея — разделение данных по слоям: Enterprise Data Warehouse (EDW) как централизованная база в 3NF и Data Marts как специализированные представления для BI.

Подход Инмона включает создание слоев: staging (временное хранилище сырых данных), ODS (Operational Data Store — операционное хранилище данных с данными в реальном времени), ref (справочники и нормализованные данные), integration (интеграционный слой с нормализованными данными в 3NF), DDS (Dimensional Data Store — денормализованные данные для аналитики) и витрины данных.

Слой integration содержит нормализованные данные в 3NF, что обеспечивает гибкость при изменениях в источниках данных. Слой DDS содержит денормализованные данные, оптимизированные для аналитических запросов. Витрины данных создаются на основе DDS для конкретных бизнес-задач.

Преимущества подхода Инмона: более надежная структура, легче адаптировать к изменениям в источниках, лучше подходит для сложных связей между данными. Недостатки: сложнее в реализации, требует больше времени на разработку, медленнее на агрегатах по сравнению с подходом Кимбалла.

## Data Vault — современная методология

Data Vault — методология моделирования хранилищ данных, разработанная Дэном Линстедтом в 2000 году для Министерства обороны США. Data Vault решает проблемы традиционных моделей: Кимбалл не справляется со сложными связями "многие ко многим", Инмон слишком сложен в поддержке, оба подхода плохо работают с изменениями в источниках данных.

Ключевая особенность Data Vault — максимальная устойчивость к изменениям. Если структура источника изменяется, в Data Vault можно добавлять новые данные без изменения существующей структуры. Это достигается через разделение сущностей, связей и атрибутов на отдельные таблицы.

Структура Data Vault состоит из трех основных типов таблиц: Hubs (хабы) — основные бизнес-сущности с неизменяемыми ключами, Links (линки) — связи между сущностями, Satellites (сателлиты) — атрибуты и их изменения во времени. Хабы содержат суррогатные ключи (Hash Key), бизнес-ключи из источника, дату загрузки и источник данных. Линки содержат связи между хабами через их ключи. Сателлиты содержат атрибуты сущностей и поддерживают историчность через даты начала и окончания действия.

Историчность в Data Vault обеспечивается сателлитами, которые хранят все версии атрибутов. При изменении атрибута создается новая запись в сателлите с новой датой начала действия. Старые записи остаются в базе для исторического анализа. Используются поля Last Seen Date для определения удаленных данных, Hash Diff для определения изменений и Record Source для отслеживания источника данных.

Программирование Data Vault включает создание процедур для загрузки данных в хабы, линки и сателлиты. Процедуры проверяют существование записей, создают новые при необходимости, обновляют сателлиты при изменениях. Важно, что данные в Data Vault не удаляются, что обеспечивает полную историчность и возможность восстановления данных.

Data Vault поддерживает реал-тайм аналитику через возможность загрузки данных в реальном времени. Изменения в структуре источников не ломают хранилище, так как новые атрибуты добавляются в сателлиты без изменения структуры хабов и линков. Это делает Data Vault особенно подходящим для больших и сложных хранилищ данных с множеством источников.

## Anchor Modeling — якорная модель

Anchor Modeling — еще одна современная методология моделирования хранилищ данных, разработанная как альтернатива Data Vault. Основная идея — максимальная нормализация данных через разделение на якоря (anchors), атрибуты (attributes) и связи (ties). Якоря представляют бизнес-сущности, атрибуты — свойства сущностей, связи — отношения между сущностями.

Главные отличия Anchor Modeling от Data Vault: более высокая нормализация, каждый атрибут хранится в отдельной таблице, более гибкая структура для изменений. Anchor Modeling лучше подходит для сценариев с частыми изменениями структуры данных и необходимостью максимальной гибкости.

## Визуализация и практическое применение

Визуализация данных в BI-инструментах (Power BI, Tableau) требует правильной подготовки данных в витринах. Модель данных из Yandex Metrika демонстрирует практическое применение хранилища для анализа веб-аналитики. Важно правильно структурировать данные для эффективной визуализации и построения интерактивных дашбордов.

Оптимизация запросов включает создание индексов на ключевых полях, использование партиционирования для больших таблиц, правильную настройку типов данных для экономии памяти. Важно учитывать, что типы данных в связанных таблицах должны совпадать для оптимальной производительности.

Автоматизация ETL-процессов через `pg_cron` позволяет настроить регулярное обновление данных без ручного вмешательства. Процедуры загрузки данных можно объединять в общие процедуры для последовательного выполнения всех этапов загрузки: Staging → Core → Marts.

## Заключение

Курс охватывает полный цикл создания хранилища данных от базовых концепций до современных методологий. Начинается с создания Staging Layer для загрузки сырых данных, переходит к Core Layer с нормализацией и многомерным моделированием по Кимбалу, рассматривает поддержку историчности и инкрементальную загрузку, изучает альтернативные подходы Инмона, Data Vault и Anchor Modeling, и завершается созданием витрин данных и визуализацией.

Каждая методология имеет свои преимущества и области применения. Кимбалл подходит для быстрой разработки и простых аналитических задач. Инмон лучше для сложных корпоративных хранилищ с множеством источников. Data Vault оптимален для больших хранилищ с частыми изменениями в источниках. Anchor Modeling подходит для максимальной гибкости и нормализации.

Ключевые принципы работы с хранилищами данных: разделение на слои (Staging, Core, Marts), нормализация данных для избежания дублирования, поддержка историчности для анализа изменений во времени, автоматизация ETL-процессов для регулярного обновления данных, оптимизация производительности через индексы и партиционирование, правильная структура данных для эффективной визуализации в BI-инструментах.

Современные хранилища данных должны быть гибкими к изменениям в источниках, поддерживать историчность данных, обеспечивать высокую производительность аналитических запросов и быть готовыми к интеграции с различными BI-инструментами. Выбор методологии зависит от конкретных требований проекта, объема данных, количества источников и частоты изменений в структуре данных.

